Epoch 1/150
484/484 [==============================] - 296s 609ms/step - loss: 56563.4414 - mae: 173.0225 - val_loss: 11319.8818 - val_mae: 84.0680 - lr: 0.0010
Epoch 2/150
484/484 [==============================] - 288s 596ms/step - loss: 6602.4844 - mae: 60.3020 - val_loss: 4811.5737 - val_mae: 54.6317 - lr: 0.0010
Epoch 3/150
484/484 [==============================] - 287s 592ms/step - loss: 2452.2493 - mae: 36.6043 - val_loss: 2902.2380 - val_mae: 39.8247 - lr: 0.0010
Epoch 4/150
484/484 [==============================] - 285s 589ms/step - loss: 1903.7726 - mae: 32.5770 - val_loss: 1600.5469 - val_mae: 31.7088 - lr: 0.0010
Epoch 5/150
484/484 [==============================] - 286s 591ms/step - loss: 1380.4374 - mae: 27.8120 - val_loss: 1765.8800 - val_mae: 31.1268 - lr: 0.0010
Epoch 6/150
484/484 [==============================] - 286s 590ms/step - loss: 1526.8945 - mae: 28.9159 - val_loss: 8214.3008 - val_mae: 78.5051 - lr: 0.0010
Epoch 7/150
484/484 [==============================] - 286s 591ms/step - loss: 1309.2993 - mae: 26.3827 - val_loss: 1847.7302 - val_mae: 35.3491 - lr: 0.0010
Epoch 8/150
484/484 [==============================] - 286s 591ms/step - loss: 991.3014 - mae: 23.5438 - val_loss: 1367.3002 - val_mae: 29.0931 - lr: 0.0010
Epoch 9/150
484/484 [==============================] - 285s 589ms/step - loss: 1015.2670 - mae: 23.9394 - val_loss: 1067.2751 - val_mae: 24.7537 - lr: 0.0010
Epoch 10/150
484/484 [==============================] - 285s 590ms/step - loss: 748.7359 - mae: 20.6583 - val_loss: 1110.9459 - val_mae: 26.6082 - lr: 0.0010
Epoch 11/150
484/484 [==============================] - 285s 589ms/step - loss: 693.6017 - mae: 19.7987 - val_loss: 1097.0470 - val_mae: 27.4802 - lr: 0.0010
Epoch 12/150
484/484 [==============================] - 282s 582ms/step - loss: 1128.5930 - mae: 24.9272 - val_loss: 12652.9336 - val_mae: 93.2629 - lr: 0.0010
Epoch 13/150
484/484 [==============================] - 280s 579ms/step - loss: 882.3790 - mae: 21.5968 - val_loss: 478.9969 - val_mae: 16.4995 - lr: 0.0010
Epoch 14/150
484/484 [==============================] - 280s 579ms/step - loss: 755.0689 - mae: 20.6312 - val_loss: 989.1882 - val_mae: 24.6029 - lr: 0.0010
Epoch 15/150
484/484 [==============================] - 281s 581ms/step - loss: 689.7313 - mae: 19.9377 - val_loss: 646.4454 - val_mae: 19.8104 - lr: 0.0010
Epoch 16/150
484/484 [==============================] - 281s 580ms/step - loss: 797.5858 - mae: 21.3001 - val_loss: 621.7422 - val_mae: 19.0851 - lr: 0.0010
Epoch 17/150
484/484 [==============================] - 281s 580ms/step - loss: 619.8321 - mae: 18.9981 - val_loss: 460.2320 - val_mae: 16.2642 - lr: 0.0010
Epoch 18/150
484/484 [==============================] - 282s 582ms/step - loss: 640.9803 - mae: 19.3306 - val_loss: 990.4063 - val_mae: 25.1139 - lr: 0.0010
Epoch 19/150
484/484 [==============================] - 281s 581ms/step - loss: 615.1355 - mae: 18.6932 - val_loss: 825.1863 - val_mae: 21.1393 - lr: 0.0010
Epoch 20/150
484/484 [==============================] - 280s 579ms/step - loss: 632.9462 - mae: 18.9365 - val_loss: 393.0537 - val_mae: 15.4301 - lr: 0.0010
Epoch 21/150
484/484 [==============================] - 282s 582ms/step - loss: 641.7610 - mae: 19.3245 - val_loss: 371.5778 - val_mae: 15.1114 - lr: 0.0010
Epoch 22/150
484/484 [==============================] - 280s 579ms/step - loss: 558.4282 - mae: 18.0983 - val_loss: 2029.4713 - val_mae: 37.8124 - lr: 0.0010
Epoch 23/150
484/484 [==============================] - 279s 576ms/step - loss: 642.0850 - mae: 19.2098 - val_loss: 793.5466 - val_mae: 22.4086 - lr: 0.0010
Epoch 24/150
484/484 [==============================] - 280s 578ms/step - loss: 573.9723 - mae: 18.1776 - val_loss: 639.6452 - val_mae: 19.3929 - lr: 0.0010
Epoch 25/150
484/484 [==============================] - 281s 581ms/step - loss: 497.2766 - mae: 17.0160 - val_loss: 869.6979 - val_mae: 22.9427 - lr: 0.0010
Epoch 26/150
484/484 [==============================] - ETA: 0s - loss: 522.7281 - mae: 17.3578
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
484/484 [==============================] - 280s 578ms/step - loss: 522.7281 - mae: 17.3578 - val_loss: 869.2696 - val_mae: 24.1817 - lr: 0.0010
Epoch 27/150
484/484 [==============================] - 280s 579ms/step - loss: 353.8587 - mae: 14.6879 - val_loss: 433.5426 - val_mae: 16.4164 - lr: 5.0000e-04
Epoch 28/150
484/484 [==============================] - 281s 582ms/step - loss: 339.9388 - mae: 14.4037 - val_loss: 359.4937 - val_mae: 14.9288 - lr: 5.0000e-04
Epoch 29/150
484/484 [==============================] - 279s 576ms/step - loss: 363.5226 - mae: 14.8483 - val_loss: 298.6726 - val_mae: 13.7392 - lr: 5.0000e-04
Epoch 30/150
484/484 [==============================] - 280s 578ms/step - loss: 352.2285 - mae: 14.6579 - val_loss: 280.6621 - val_mae: 13.2897 - lr: 5.0000e-04
Epoch 31/150
484/484 [==============================] - 279s 576ms/step - loss: 367.2794 - mae: 14.9183 - val_loss: 769.6911 - val_mae: 21.6544 - lr: 5.0000e-04
Epoch 32/150
484/484 [==============================] - 279s 577ms/step - loss: 378.6674 - mae: 15.0837 - val_loss: 278.0710 - val_mae: 13.2731 - lr: 5.0000e-04
Epoch 33/150
484/484 [==============================] - 279s 577ms/step - loss: 368.2682 - mae: 14.9100 - val_loss: 278.1942 - val_mae: 13.2068 - lr: 5.0000e-04
Epoch 34/150
484/484 [==============================] - 279s 577ms/step - loss: 347.8211 - mae: 14.5017 - val_loss: 257.7230 - val_mae: 12.7025 - lr: 5.0000e-04
Epoch 35/150
484/484 [==============================] - 281s 580ms/step - loss: 353.4459 - mae: 14.6729 - val_loss: 264.8796 - val_mae: 12.8832 - lr: 5.0000e-04
Epoch 36/150
484/484 [==============================] - 279s 577ms/step - loss: 361.5402 - mae: 14.7592 - val_loss: 282.0796 - val_mae: 13.4021 - lr: 5.0000e-04
Epoch 37/150
484/484 [==============================] - 280s 579ms/step - loss: 318.3341 - mae: 13.9350 - val_loss: 271.3748 - val_mae: 13.1992 - lr: 5.0000e-04
Epoch 38/150
484/484 [==============================] - 280s 578ms/step - loss: 319.6917 - mae: 13.9826 - val_loss: 581.4293 - val_mae: 19.0553 - lr: 5.0000e-04
Epoch 39/150
484/484 [==============================] - ETA: 0s - loss: 364.8705 - mae: 14.7678
Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
484/484 [==============================] - 280s 578ms/step - loss: 364.8705 - mae: 14.7678 - val_loss: 327.4841 - val_mae: 14.2515 - lr: 5.0000e-04
Epoch 40/150
484/484 [==============================] - 279s 577ms/step - loss: 246.7076 - mae: 12.4560 - val_loss: 274.8948 - val_mae: 12.9903 - lr: 2.5000e-04
Epoch 41/150
484/484 [==============================] - 279s 577ms/step - loss: 248.7789 - mae: 12.4738 - val_loss: 226.8361 - val_mae: 11.9577 - lr: 2.5000e-04
Epoch 42/150
484/484 [==============================] - 280s 578ms/step - loss: 255.4177 - mae: 12.6097 - val_loss: 370.3164 - val_mae: 15.0186 - lr: 2.5000e-04
Epoch 43/150
484/484 [==============================] - 279s 577ms/step - loss: 248.9360 - mae: 12.4474 - val_loss: 254.4501 - val_mae: 12.5846 - lr: 2.5000e-04
Epoch 44/150
484/484 [==============================] - 279s 577ms/step - loss: 256.1745 - mae: 12.6219 - val_loss: 238.0848 - val_mae: 12.2588 - lr: 2.5000e-04
Epoch 45/150
484/484 [==============================] - 280s 578ms/step - loss: 247.5151 - mae: 12.4011 - val_loss: 215.7646 - val_mae: 11.7371 - lr: 2.5000e-04
Epoch 46/150
484/484 [==============================] - 279s 577ms/step - loss: 243.2573 - mae: 12.2482 - val_loss: 236.6294 - val_mae: 12.2409 - lr: 2.5000e-04
Epoch 47/150
484/484 [==============================] - 283s 585ms/step - loss: 246.9084 - mae: 12.3561 - val_loss: 215.0295 - val_mae: 11.7629 - lr: 2.5000e-04
Epoch 48/150
484/484 [==============================] - 285s 589ms/step - loss: 262.5650 - mae: 12.7057 - val_loss: 247.9180 - val_mae: 12.5367 - lr: 2.5000e-04
Epoch 49/150
484/484 [==============================] - 285s 588ms/step - loss: 260.4680 - mae: 12.6856 - val_loss: 309.1936 - val_mae: 13.8074 - lr: 2.5000e-04
Epoch 50/150
484/484 [==============================] - 285s 590ms/step - loss: 243.5942 - mae: 12.2789 - val_loss: 248.2036 - val_mae: 12.2640 - lr: 2.5000e-04
Epoch 51/150
484/484 [==============================] - 284s 588ms/step - loss: 248.2232 - mae: 12.4049 - val_loss: 226.2819 - val_mae: 11.9771 - lr: 2.5000e-04
Epoch 52/150
484/484 [==============================] - ETA: 0s - loss: 242.6683 - mae: 12.2492
Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
484/484 [==============================] - 462s 955ms/step - loss: 242.6683 - mae: 12.2492 - val_loss: 221.7760 - val_mae: 11.8261 - lr: 2.5000e-04
Epoch 53/150
484/484 [==============================] - 456s 942ms/step - loss: 207.3888 - mae: 11.3835 - val_loss: 199.6214 - val_mae: 11.3452 - lr: 1.2500e-04
Epoch 54/150
484/484 [==============================] - 467s 964ms/step - loss: 206.1084 - mae: 11.3774 - val_loss: 246.9846 - val_mae: 12.3895 - lr: 1.2500e-04
Epoch 55/150
484/484 [==============================] - 463s 956ms/step - loss: 206.2565 - mae: 11.3727 - val_loss: 207.0511 - val_mae: 11.4478 - lr: 1.2500e-04
Epoch 56/150
484/484 [==============================] - 493s 1s/step - loss: 208.8960 - mae: 11.4095 - val_loss: 207.0040 - val_mae: 11.4464 - lr: 1.2500e-04
Epoch 57/150
484/484 [==============================] - 502s 1s/step - loss: 212.8959 - mae: 11.5171 - val_loss: 338.7196 - val_mae: 14.3962 - lr: 1.2500e-04
Epoch 58/150
484/484 [==============================] - ETA: 0s - loss: 202.8741 - mae: 11.2779
Epoch 58: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
484/484 [==============================] - 516s 1s/step - loss: 202.8741 - mae: 11.2779 - val_loss: 237.4504 - val_mae: 12.1400 - lr: 1.2500e-04
Epoch 59/150
484/484 [==============================] - 522s 1s/step - loss: 187.1920 - mae: 10.8712 - val_loss: 208.9382 - val_mae: 11.4431 - lr: 6.2500e-05
Epoch 60/150
484/484 [==============================] - 481s 995ms/step - loss: 184.8662 - mae: 10.8166 - val_loss: 201.3837 - val_mae: 11.2826 - lr: 6.2500e-05
Epoch 61/150
484/484 [==============================] - 376s 777ms/step - loss: 188.8379 - mae: 10.8952 - val_loss: 203.0967 - val_mae: 11.2256 - lr: 6.2500e-05
Epoch 62/150
484/484 [==============================] - 289s 596ms/step - loss: 188.4054 - mae: 10.8764 - val_loss: 199.3129 - val_mae: 11.1260 - lr: 6.2500e-05
Epoch 63/150
484/484 [==============================] - 298s 616ms/step - loss: 185.4803 - mae: 10.7964 - val_loss: 195.0507 - val_mae: 11.1419 - lr: 6.2500e-05
Epoch 64/150
484/484 [==============================] - 289s 597ms/step - loss: 184.6040 - mae: 10.7724 - val_loss: 209.1935 - val_mae: 11.4683 - lr: 6.2500e-05
Epoch 65/150
484/484 [==============================] - 286s 591ms/step - loss: 186.9401 - mae: 10.8222 - val_loss: 213.5824 - val_mae: 11.4725 - lr: 6.2500e-05
Epoch 66/150
484/484 [==============================] - 306s 632ms/step - loss: 186.0382 - mae: 10.8177 - val_loss: 220.3637 - val_mae: 11.6812 - lr: 6.2500e-05
Epoch 67/150
484/484 [==============================] - 310s 641ms/step - loss: 185.0045 - mae: 10.7737 - val_loss: 196.4835 - val_mae: 11.1590 - lr: 6.2500e-05
Epoch 68/150
484/484 [==============================] - ETA: 0s - loss: 187.0021 - mae: 10.8331
Epoch 68: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
484/484 [==============================] - 312s 644ms/step - loss: 187.0021 - mae: 10.8331 - val_loss: 198.0400 - val_mae: 11.1740 - lr: 6.2500e-05
Epoch 69/150
484/484 [==============================] - 314s 649ms/step - loss: 174.2715 - mae: 10.4764 - val_loss: 192.1128 - val_mae: 10.9860 - lr: 3.1250e-05
Epoch 70/150
484/484 [==============================] - 322s 666ms/step - loss: 174.1939 - mae: 10.4779 - val_loss: 216.9016 - val_mae: 11.6419 - lr: 3.1250e-05
Epoch 71/150
484/484 [==============================] - 311s 642ms/step - loss: 174.0874 - mae: 10.4637 - val_loss: 184.5769 - val_mae: 10.8220 - lr: 3.1250e-05
Epoch 72/150
484/484 [==============================] - 313s 647ms/step - loss: 175.6722 - mae: 10.5200 - val_loss: 194.2031 - val_mae: 10.9828 - lr: 3.1250e-05
Epoch 73/150
484/484 [==============================] - 318s 657ms/step - loss: 174.9357 - mae: 10.4772 - val_loss: 227.4617 - val_mae: 11.8431 - lr: 3.1250e-05
Epoch 74/150
484/484 [==============================] - 331s 682ms/step - loss: 174.1800 - mae: 10.4733 - val_loss: 189.9795 - val_mae: 11.0453 - lr: 3.1250e-05
Epoch 75/150
484/484 [==============================] - 339s 700ms/step - loss: 174.4426 - mae: 10.4894 - val_loss: 190.5222 - val_mae: 10.9288 - lr: 3.1250e-05
Epoch 76/150
484/484 [==============================] - ETA: 0s - loss: 175.3215 - mae: 10.4822
Epoch 76: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
484/484 [==============================] - 341s 704ms/step - loss: 175.3215 - mae: 10.4822 - val_loss: 190.8387 - val_mae: 10.9830 - lr: 3.1250e-05
Epoch 77/150
484/484 [==============================] - 330s 683ms/step - loss: 168.6781 - mae: 10.3009 - val_loss: 186.4976 - val_mae: 10.7940 - lr: 1.5625e-05
Epoch 78/150
484/484 [==============================] - 318s 658ms/step - loss: 168.3040 - mae: 10.2914 - val_loss: 189.3820 - val_mae: 10.9134 - lr: 1.5625e-05
Epoch 79/150
484/484 [==============================] - 323s 668ms/step - loss: 169.5496 - mae: 10.3201 - val_loss: 184.2769 - val_mae: 10.7668 - lr: 1.5625e-05
Epoch 80/150
484/484 [==============================] - 335s 691ms/step - loss: 168.1808 - mae: 10.2859 - val_loss: 183.6022 - val_mae: 10.7416 - lr: 1.5625e-05
Epoch 81/150
484/484 [==============================] - 295s 609ms/step - loss: 167.3367 - mae: 10.2615 - val_loss: 183.1716 - val_mae: 10.7477 - lr: 1.5625e-05
Epoch 82/150
484/484 [==============================] - 317s 654ms/step - loss: 168.0501 - mae: 10.2783 - val_loss: 195.9529 - val_mae: 11.0250 - lr: 1.5625e-05
Epoch 83/150
484/484 [==============================] - 320s 661ms/step - loss: 167.5785 - mae: 10.2722 - val_loss: 185.4898 - val_mae: 10.7725 - lr: 1.5625e-05
Epoch 84/150
484/484 [==============================] - 310s 640ms/step - loss: 169.0582 - mae: 10.3126 - val_loss: 184.0766 - val_mae: 10.7509 - lr: 1.5625e-05
Epoch 85/150
484/484 [==============================] - 292s 602ms/step - loss: 168.2125 - mae: 10.2927 - val_loss: 183.2359 - val_mae: 10.7473 - lr: 1.5625e-05
Epoch 86/150
484/484 [==============================] - ETA: 0s - loss: 167.1061 - mae: 10.2534
Epoch 86: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
484/484 [==============================] - 316s 654ms/step - loss: 167.1061 - mae: 10.2534 - val_loss: 187.8793 - val_mae: 10.8474 - lr: 1.5625e-05
Epoch 87/150
484/484 [==============================] - 302s 625ms/step - loss: 165.2711 - mae: 10.1904 - val_loss: 188.5721 - val_mae: 10.8314 - lr: 7.8125e-06
Epoch 88/150
484/484 [==============================] - 307s 634ms/step - loss: 164.6332 - mae: 10.1727 - val_loss: 181.6637 - val_mae: 10.6986 - lr: 7.8125e-06
Epoch 89/150
484/484 [==============================] - 323s 668ms/step - loss: 164.6596 - mae: 10.1751 - val_loss: 182.6294 - val_mae: 10.7259 - lr: 7.8125e-06
Epoch 90/150
484/484 [==============================] - 292s 603ms/step - loss: 164.9721 - mae: 10.1888 - val_loss: 183.0432 - val_mae: 10.7126 - lr: 7.8125e-06
Epoch 91/150
484/484 [==============================] - 329s 679ms/step - loss: 164.6499 - mae: 10.1717 - val_loss: 183.1742 - val_mae: 10.7163 - lr: 7.8125e-06
Epoch 92/150
484/484 [==============================] - 301s 622ms/step - loss: 164.6345 - mae: 10.1669 - val_loss: 183.9553 - val_mae: 10.7388 - lr: 7.8125e-06
Epoch 93/150
484/484 [==============================] - ETA: 0s - loss: 164.5966 - mae: 10.1796
Epoch 93: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
484/484 [==============================] - 294s 607ms/step - loss: 164.5966 - mae: 10.1796 - val_loss: 184.4736 - val_mae: 10.7430 - lr: 7.8125e-06
Epoch 94/150
484/484 [==============================] - 294s 607ms/step - loss: 163.1274 - mae: 10.1187 - val_loss: 182.7179 - val_mae: 10.7002 - lr: 3.9063e-06
Epoch 95/150
484/484 [==============================] - 306s 632ms/step - loss: 163.0231 - mae: 10.1240 - val_loss: 182.1788 - val_mae: 10.7110 - lr: 3.9063e-06
Epoch 96/150
484/484 [==============================] - 364s 752ms/step - loss: 163.2656 - mae: 10.1307 - val_loss: 181.5470 - val_mae: 10.6914 - lr: 3.9063e-06
Epoch 97/150
484/484 [==============================] - 669s 1s/step - loss: 162.7863 - mae: 10.1215 - val_loss: 181.9079 - val_mae: 10.6970 - lr: 3.9063e-06
Epoch 98/150
484/484 [==============================] - 659s 1s/step - loss: 162.8704 - mae: 10.1082 - val_loss: 182.6405 - val_mae: 10.7271 - lr: 3.9063e-06
Epoch 99/150
484/484 [==============================] - 487s 1s/step - loss: 163.2148 - mae: 10.1273 - val_loss: 182.0065 - val_mae: 10.7052 - lr: 3.9063e-06
Epoch 100/150
484/484 [==============================] - 300s 620ms/step - loss: 163.1433 - mae: 10.1266 - val_loss: 181.7250 - val_mae: 10.6915 - lr: 3.9063e-06
Epoch 101/150
484/484 [==============================] - ETA: 0s - loss: 163.2637 - mae: 10.1290
Epoch 101: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
484/484 [==============================] - 296s 612ms/step - loss: 163.2637 - mae: 10.1290 - val_loss: 181.7704 - val_mae: 10.7146 - lr: 3.9063e-06
Epoch 102/150
484/484 [==============================] - 296s 613ms/step - loss: 162.1853 - mae: 10.0981 - val_loss: 181.5017 - val_mae: 10.6887 - lr: 1.9531e-06
Epoch 103/150
484/484 [==============================] - 293s 605ms/step - loss: 162.3320 - mae: 10.1070 - val_loss: 181.2885 - val_mae: 10.6830 - lr: 1.9531e-06
Epoch 104/150
484/484 [==============================] - 310s 640ms/step - loss: 162.0251 - mae: 10.0925 - val_loss: 181.4238 - val_mae: 10.6881 - lr: 1.9531e-06
Epoch 105/150
484/484 [==============================] - 288s 596ms/step - loss: 162.0980 - mae: 10.0958 - val_loss: 181.5594 - val_mae: 10.6939 - lr: 1.9531e-06
Epoch 106/150
484/484 [==============================] - 319s 660ms/step - loss: 162.2329 - mae: 10.1002 - val_loss: 181.2378 - val_mae: 10.6827 - lr: 1.9531e-06
Epoch 107/150
484/484 [==============================] - 309s 638ms/step - loss: 161.8845 - mae: 10.0832 - val_loss: 182.5871 - val_mae: 10.7071 - lr: 1.9531e-06
Epoch 108/150
484/484 [==============================] - 310s 641ms/step - loss: 162.0263 - mae: 10.0932 - val_loss: 181.2081 - val_mae: 10.6817 - lr: 1.9531e-06
Epoch 109/150
484/484 [==============================] - 294s 609ms/step - loss: 162.0504 - mae: 10.0901 - val_loss: 181.7726 - val_mae: 10.7093 - lr: 1.9531e-06
Epoch 110/150
484/484 [==============================] - 300s 621ms/step - loss: 162.0734 - mae: 10.0938 - val_loss: 181.8639 - val_mae: 10.6836 - lr: 1.9531e-06
Epoch 111/150
484/484 [==============================] - 313s 647ms/step - loss: 162.0046 - mae: 10.0914 - val_loss: 181.1981 - val_mae: 10.6781 - lr: 1.9531e-06
Epoch 112/150
484/484 [==============================] - 345s 714ms/step - loss: 161.9264 - mae: 10.0913 - val_loss: 181.3581 - val_mae: 10.6836 - lr: 1.9531e-06
Epoch 113/150
484/484 [==============================] - 343s 709ms/step - loss: 161.8821 - mae: 10.0870 - val_loss: 181.6781 - val_mae: 10.7085 - lr: 1.9531e-06
Epoch 114/150
484/484 [==============================] - 294s 608ms/step - loss: 162.0789 - mae: 10.0934 - val_loss: 181.1628 - val_mae: 10.6751 - lr: 1.9531e-06
Epoch 115/150
484/484 [==============================] - 332s 686ms/step - loss: 162.0553 - mae: 10.0948 - val_loss: 182.4099 - val_mae: 10.6887 - lr: 1.9531e-06
Epoch 116/150
484/484 [==============================] - 312s 644ms/step - loss: 161.9475 - mae: 10.0847 - val_loss: 181.3212 - val_mae: 10.6779 - lr: 1.9531e-06
Epoch 117/150
484/484 [==============================] - 303s 625ms/step - loss: 161.9421 - mae: 10.0865 - val_loss: 183.4759 - val_mae: 10.7153 - lr: 1.9531e-06
Epoch 118/150
484/484 [==============================] - 300s 619ms/step - loss: 162.0194 - mae: 10.0873 - val_loss: 181.1232 - val_mae: 10.6729 - lr: 1.9531e-06
Epoch 119/150
484/484 [==============================] - 297s 613ms/step - loss: 161.8549 - mae: 10.0827 - val_loss: 181.1432 - val_mae: 10.6750 - lr: 1.9531e-06
Epoch 120/150
484/484 [==============================] - 291s 601ms/step - loss: 161.8415 - mae: 10.0776 - val_loss: 183.4073 - val_mae: 10.7176 - lr: 1.9531e-06
Epoch 121/150
484/484 [==============================] - 295s 609ms/step - loss: 161.8578 - mae: 10.0884 - val_loss: 181.8967 - val_mae: 10.6835 - lr: 1.9531e-06
Epoch 122/150
484/484 [==============================] - 293s 605ms/step - loss: 161.8687 - mae: 10.0826 - val_loss: 181.7108 - val_mae: 10.6870 - lr: 1.9531e-06
Epoch 123/150
484/484 [==============================] - ETA: 0s - loss: 161.8008 - mae: 10.0807
Epoch 123: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
484/484 [==============================] - 299s 618ms/step - loss: 161.8008 - mae: 10.0807 - val_loss: 181.1303 - val_mae: 10.6821 - lr: 1.9531e-06
Epoch 124/150
484/484 [==============================] - 307s 635ms/step - loss: 161.3861 - mae: 10.0720 - val_loss: 180.9306 - val_mae: 10.6763 - lr: 9.7656e-07
Epoch 125/150
484/484 [==============================] - 290s 600ms/step - loss: 161.2610 - mae: 10.0649 - val_loss: 182.7750 - val_mae: 10.6971 - lr: 9.7656e-07
Epoch 126/150
484/484 [==============================] - 302s 625ms/step - loss: 161.4488 - mae: 10.0712 - val_loss: 181.0210 - val_mae: 10.6762 - lr: 9.7656e-07
Epoch 127/150
484/484 [==============================] - 305s 631ms/step - loss: 161.3717 - mae: 10.0651 - val_loss: 181.1320 - val_mae: 10.6751 - lr: 9.7656e-07
Epoch 128/150
484/484 [==============================] - 301s 622ms/step - loss: 161.4035 - mae: 10.0649 - val_loss: 181.2042 - val_mae: 10.6842 - lr: 9.7656e-07
Epoch 129/150
484/484 [==============================] - ETA: 0s - loss: 161.4053 - mae: 10.0708
Epoch 129: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.
484/484 [==============================] - 295s 610ms/step - loss: 161.4053 - mae: 10.0708 - val_loss: 181.2902 - val_mae: 10.6748 - lr: 9.7656e-07
Epoch 130/150
484/484 [==============================] - 311s 642ms/step - loss: 161.2616 - mae: 10.0634 - val_loss: 181.0877 - val_mae: 10.6793 - lr: 4.8828e-07
Epoch 131/150
484/484 [==============================] - 322s 666ms/step - loss: 161.2034 - mae: 10.0606 - val_loss: 181.2080 - val_mae: 10.6724 - lr: 4.8828e-07
Epoch 132/150
484/484 [==============================] - 300s 620ms/step - loss: 161.1641 - mae: 10.0628 - val_loss: 181.0003 - val_mae: 10.6745 - lr: 4.8828e-07
Epoch 133/150
484/484 [==============================] - 320s 661ms/step - loss: 161.1344 - mae: 10.0591 - val_loss: 181.0040 - val_mae: 10.6736 - lr: 4.8828e-07
Epoch 134/150
484/484 [==============================] - ETA: 0s - loss: 161.1591 - mae: 10.0609
Epoch 134: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.
484/484 [==============================] - 308s 637ms/step - loss: 161.1591 - mae: 10.0609 - val_loss: 180.9975 - val_mae: 10.6752 - lr: 4.8828e-07